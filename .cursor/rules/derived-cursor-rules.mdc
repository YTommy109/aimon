---
description: AI rules derived by SpecStory from the project AI interaction history
globs: *
---

## HEADERS

## PROJECT RULES

### General Rules
- The AI coding assistant must adhere to all instructions provided in this file.
- The AI coding assistant should always ask for clarification when uncertain.
- The AI coding assistant should provide multiple options when possible.
- **ALWAYS** implement the minimum required to address the problem. Avoid adding forward-looking code or features. If code is unused, it should be removed.
- **Status Updates:** Provide status updates before tool calls and after edits, particularly tests and builds. Fix any failures before marking tasks complete.
- **Code Review Focus:** When performing code reviews, prioritize identifying areas for simplification and commonization.

### pyproject.toml Modification Rules

- **NEVER** directly modify the `pyproject.toml` file without explicit user consent.
- **ALWAYS** propose dependency additions or removals to the user for approval before implementing them.
- **ALWAYS** consult the user before making any configuration changes to `pyproject.toml`.
- If a change to `pyproject.toml` is needed, the AI assistant should first request permission from the user.

### File Deletion Rules
- Before deleting a file, **ALWAYS** confirm that:
    - The file contains only comments or placeholder code.
    - The file is not referenced or imported anywhere in the project.
    - The current implementation does not rely on the file's functionality.
- If all conditions are met, the AI assistant MAY delete the file after notifying the user.

### Command Execution Rules

- When executing commands using `subprocess.run`, the AI coding assistant must ensure the command includes the target directory path.
- The `{source_path}` placeholder MUST be used within the command string to represent the project's source directory.
- The AI coding assistant must validate that the command string contains the `{source_path}` placeholder before execution, if the placeholder is present in the command. If the placeholder is not present, the placeholder is not present, the command will be executed as is.
- The working directory for command execution should remain the Streamlit app's launch directory. Do not set `cwd` in `subprocess.run`.
- AI tool commands should be configured to specify the target directory using the `{source_path}` placeholder (e.g., `ait analyze {source_path}`).
- The AI coding assistant should ensure that commands like `pushd` and `popd` are included in the list of allowed command prefixes if they are used in the execution of AI tools.
- When using `subprocess` to execute commands, be aware that login shells (e.g., `.zshrc`, `.bashrc`) are not automatically loaded. To ensure environment variables are correctly set, consider the following approaches:
    - **Direct Environment Variable Setting:** Set environment variables directly in the `env` argument of `subprocess.run`.
    - **Loading `.envrc`:** Load environment variables from the `.envrc` file and pass them to `subprocess.run`.
    - **Shell Execution:** Execute commands through the shell to force loading of login shell configurations (e.g., using `zsh -c "source ~/.zshrc && command"`).

### Data Serialization Rules

- When using Pydantic's `model_dump` method for serializing models to JSON:
    - The AI coding assistant should be aware that `@property` attributes are included by default.
    - To exclude `@property` attributes, use the `exclude` parameter in `model_dump`. For example: `model_dump(mode='json', exclude={'attribute_name'})`.
    - This is particularly important for calculated properties that should not be persisted in the data store (e.g., a `status` property derived from date fields).
- When saving project data, exclude the `status` attribute from the `Project` model when using `model_dump` to prevent it from being written to `projects.json`.

### ValueError Handling
- The AI coding assistant should be aware of the common causes of `ValueError` in the project:
    - **Empty AI Tool Command:** When creating or updating an AI tool, the `command` field cannot be an empty string or contain only whitespace.
    - **Invalid Project ID:** The project ID must be a valid UUID format.
    - **AI Tool Not Found:** When retrieving an AI tool by ID, the ID must exist in the repository.

### Internal AI Tool Rules
- The project will use internal AI tools instead of relying solely on external AI tools.
- The following LLMs should be supported through `lightllm`: OpenAI, Gemini, and internal LLMs.
- The project will initially provide two internal AI tools: "OVERVIEW" (summarization) and "REVIEW". These should be defined using `StrEnum`.
- The `ai_tools.json` file is no longer required. The `Project` data model will hold the `OVERVIEW` or `REVIEW` identifier instead of an AI tool ID.

### Environment Configuration Rules
- The project should use `.env` files for environment configuration instead of `direnv`.
    - `.env`: Production environment
    - `.env.dev`: Development environment
    - `.env.test`: Test environment
- The environment (prod, dev, test) should be switchable using:
    - The `ENV` environment variable.
    - A Streamlit startup option (`-- --app-env dev|test|prod`). The Streamlit startup option takes precedence over the `ENV` environment variable. If neither is specified, the environment defaults to `prod`.
- **There should only be one `.env.example` file** because the variables are the same across prod, dev, and test environments. This file is named `env_sample`.

### Change Planning Rules
- Before implementing significant architectural changes, a `plan.md` document should be created. This plan should outline:
    - Phases and tasks for the transition.
    - Risks and acceptance criteria for each task.
    - Strategies for migrating from existing implementations (e.g., `ai_tools.json`).
- The `plan.md` file should be formatted using `markdownlint-cli2` and must pass all linting checks before implementation begins. The `just mdlint plan.md` command should be used for this purpose.
- The plan should prioritize incremental development and frequent execution of `just test-all` to maintain code quality.
- The plan should consider the order of environment variable precedence: Streamlit startup options (`--app-env`) take precedence over the `ENV` environment variable, which defaults to `prod` if neither is specified.
- The plan should address the transition from external AI tool commands (via AITool) to internal `ToolType StrEnum`.
- The AI coding assistant should add checkboxes to `plan.md` to visualize progress.

### Environment File Handling Rules

- To avoid committing sensitive information, actual `.env` files (`.env`, `.env.dev`, `.env.test`) should remain untracked by Git.
- Instead of directly modifying `.gitignore`, create `env_sample` file with commented-out placeholders for environment variables. This example file can be committed to the repository as a template.
- Before modifying `.gitignore` to exclude `.env` files, always seek explicit user confirmation.

### Linting and Formatting Rules

- After editing `.py` files, the AI coding assistant should run `just ruff path='<file_path>'` to format and lint the file.
- The AI coding assistant should use `just` for all development tasks, avoiding direct calls to `ruff` or other formatters/linters.
- When fixing type errors, the AI coding assistant should choose the most minimal fix possible (e.g., casting to a string using `str(args.app_env)`).

### Plan.md Rules
- The AI coding assistant should add checkboxes to `plan.md` to visualize progress.
- The AI coding assistant should use two spaces for indentation in nested lists.
- The AI coding assistant should always update `plan.md` to reflect the current project status.
- The AI coding assistant should use the `plan.md` file to determine the current project status and remaining tasks.
- The AI coding assistant should use `plan.md` to guide the development process and prioritize tasks based on the plan's current state.

### Vulture Configuration Rules

- When running `vulture`, target application code (e.g., `app` and `pages`) and exclude test directories (e.g., `tests`, `tests_e2e`) to accurately identify unused code in the application.
- Set the minimum confidence level for `vulture` to ensure that potential unused code is detected, while minimizing false positives.
- Add any excluded files to the `exclude` list in `pyproject.toml` under the `[tool.vulture]` section.
- Add any ignored names to the `ignore_names` list in `pyproject.toml` under the `[tool.vulture]` section.
- **Unused code should be removed** to maintain a minimal implementation. Future-looking code is not allowed. If code is unused, delete it.

### ENVIRONMENT VARIABLE RULES

- The application environment variables should be configured such that:
    - Streamlit startup options (`--app-env dev|test|prod`) take precedence over the `ENV` environment variable.
    - The `ENV` environment variable defaults to `prod` if neither the startup option nor the `ENV` variable is specified.
- When `ENV` is set to 'test', `data_dir_path` returns `DATA_DIR_TEST` if it's defined, overriding `DATA_DIR`. This ensures backward compatibility.
- When `ENV` is set to 'test', the application should use test-specific configurations or data directories.

### `.cursorrules` SPECIFIC RULES

- The AI coding assistant should respect rules defined in `.cursor/rules` or `.cursorrules` to restrict its actions.
- Prefer the `.cursor/rules` directory for rule files; `.cursorrules` is considered an older approach.
- The current preferred way to define rules is to place `{filename}.mdc` files under the `.cursor/rules` directory. These files specify rules in a structured format.

### GIT ATTRIBUTES RULES

- Important configuration files should be treated as read-only within Git.
- Example: `pyproject.toml -text` in `.gitattributes` marks the file as read-only.

### Mypy Configuration Rules
- When encountering mypy errors related to the internal AI tool transition (e.g., `AITool`, `AIToolID` references in older code), and following the plan to incrementally transition and eventually remove the old code:
    - Modify `pyproject.toml` to exclude the relevant files or modules from mypy checks temporarily. This allows focusing on current development without being blocked by legacy code.
    - Add an override in the `[tool.mypy.overrides]` section of `pyproject.toml` to ignore `ignore_missing_imports` and set `disallow_any_unimported = false` for the affected modules.
    - If necessary, add `warn_return_any = false` to the override to suppress "Returning Any" warnings if functions are known to return `Any` during the transition.
    - **IMPORTANT**: Document the reason for the mypy override in `pyproject.toml` with a comment indicating that the override is temporary and related to the internal AI tool transition. The comment should also mention that the affected code will be removed or refactored in a later stage.

### Step 5 (Post-Cleanup) of Internal AI Tool Transition
- When performing Step 5 (Post-Cleanup) of the internal AI tool transition, which includes removing `ai_tools.json`, `AITool`, `JsonAIToolRepository`, and `AIToolService`:
    - **Confirm Code Usage:** Before deleting these components, confirm that they are no longer actively used within the project.
    - **Address Import Errors:** If import errors arise due to the removal of these components, remove the corresponding imports in the affected files.
    - **Compatibility Code Removal:** Remove any compatibility code related to the old AI tool system, such as code handling the `ai_tool` field in project data.
    - **Data Migration:** Ensure that project data is migrated to the new format (`ToolType`) before removing compatibility code. This may involve data transformation scripts or manual updates.
    - **Test Thoroughly:** After removing these components, run comprehensive tests to ensure that all functionalities are working as expected.

### Task Execution Based on Plan
- When the user asks to execute the rest of the plan, the AI coding assistant should:
    - Check `plan.md` to determine the current project status and remaining tasks.
    - Proceed with the tasks outlined in `plan.md` in the order they appear.

### E2E Test Updates
- When updating E2E tests:
    - Replace `page_with_ai_tool_test_data` with `page_with_app`.
    - Remove the lines that navigate to `/AI_Tool_Management` because the AI Tool Management page is no longer needed.
    - Replace the label `AIツールを選択` with `ツールタイプを選択`.

### Data Migration Rules
- During the internal AI tool transition, when the `Project` model's `tool` field is mandatory, and existing data only contains the `ai_tool` field:
    - **Data Transformation:** Implement a data migration process in the repository to convert the existing `ai_tool` field to the new `tool` field.
    - **Field Mapping:** Ensure a correct mapping between the values of `ai_tool` and the corresponding `ToolType` enum values.
    - **Data Consistency:** Before removing the `ai_tool` field, ensure all existing data has been migrated to use the `tool` field.
- After the `ai_tool` to `tool` migration is complete and the `ai_tool` field is no longer needed:
    - Remove the `ai_tool` field from the `Project` model.
    - Remove the code related to handling the `ai_tool` field in the repository's data normalization process.

### Streamlit Startup Options Rules
- When using Streamlit, custom options must be passed after `--` in the `justfile`. For example: `streamlit run app.py -- --app-env dev`

### Post ai_tool Field Removal

- After confirming that `projects.json` has been updated and the `ai_tool` field is no longer needed:
    - Remove all code related to handling the `ai_tool` field in `project_repository.py`.
    - Ensure the `Project` model no longer contains the `ai_tool` field.
    - Verify that all tests pass after removing the code.

### LLM Handling Rules

- The initial implementation of LLM calling is a stub (placeholder).
- The project should support LLMs via `lightllm`, including OpenAI, Gemini, and internal LLMs.
- An `LLMClient` (lightllm) is intended to be used to call LLMs. However, the current implementation may be mocked.
- **LLM Provider Configuration:**
    - The `LLM_PROVIDER` environment variable specifies the LLM provider to use.
    - Valid values are `openai`, `gemini`, and `internal` (case-sensitive).
    - If `LLM_PROVIDER` is not set, the default provider is `openai`.
    - Invalid values for `LLM_PROVIDER` will result in a `ValueError` during `LLMClient` initialization, wrapped in a `RuntimeError`.
    - **Required environment variables per provider:**
        - **openai**: `OPENAI_API_KEY` (required), `OPENAI_API_BASE` (optional), `OPENAI_MODEL` (optional, defaults to `gpt-3.5-turbo`)
        - **gemini**: `GEMINI_API_KEY` (required), `GEMINI_API_BASE` (optional), `GEMINI_MODEL` (optional, defaults to `gemini-pro`)
        - **internal**: `INTERNAL_LLM_ENDPOINT` (required), `INTERNAL_LLM_API_KEY` (optional), `INTERNAL_LLM_MODEL` (optional, defaults to `internal-model`)

### Development Workflow Rules

- For each task in the implementation phases (as defined in `plan.md`):
    - After completing a task, the AI coding assistant MUST run `just test-all` to ensure code quality.
    - If `uv sync` has been run, ensure that development dependencies are installed by running `uv sync --extra dev`.

### Dependency Management Rules

- Before adding any new dependencies to `pyproject.toml`, the AI coding assistant MUST:
    - Consult `plan.md` to understand the project's requirements and planned dependencies.
    - Propose the addition to the user, explaining the rationale and alternatives (e.g., `lightllm` vs. `litellm`).
    - Await user approval before making any changes to `pyproject.toml`.

### LLMClient Implementation Rules

- The `LLMClient` class should be placed in the `app/utils` directory.
- When implementing the `LLMClient`, use the following structure:
    - Abstract base class `LLMProvider`.
    - Concrete provider classes (`OpenAIProvider`, `GeminiProvider`, `InternalLLMProvider`).
    - Unified client `LLMClient`.
- The `LLMClient` should support switching providers at runtime.
- The `LLMClient` should support asynchronous text generation.
- Ensure proper error handling for unsupported providers.
- When the `LLMClient` or provider classes encounter an exception during API calls, raise a `RuntimeError` with a descriptive message. Include the original exception information using `raise ... from err`.
- When initializing providers within the `_initialize_provider` method of the `LLMClient`, retrieve API keys and other necessary credentials from environment variables. Raise a `ValueError` with a descriptive message if any required environment variables are not set.
- When defining the `provider_name` for the `LLMClient`, define `provider_name` as a `StrEnum` and replace the `if/elif` statements with a `match` statement.
- For backward compatibility, the externally accessible `provider_name` should remain a string property, while internally the enum should be used.
- It is no longer necessary to have `_initialize_specific_provider`. Instead, the `match` statement should be performed directly in `_initialize_provider`.
- The `assert` and default `raise` statements in the original `_initialize_provider` are no longer needed.
- When configuring the `InternalLLMProvider`, the `_call_api` method should explicitly specify `api_base` and `api_key` when calling `litellm.completion`.
- The `_call_api` method of the `InternalLLMProvider` should add the `custom/` prefix to the model name for internal LLMs, as required by `litellm`.

### Testing Rules for LLMClient

- Create tests for the `LLMClient` class and its provider classes.
- Test the initialization of each provider.
- Test the text generation functionality of each provider.
- Test the provider switching functionality of the `LLMClient`.
- Ensure that tests cover cases where the provider is not initialized or is unsupported.
- Exclude the newly created `LLMClient` class from Vulture checks until it is fully implemented and used.
- All tests must pass, and code coverage should remain above 85%.
- When testing API calls via `LLMClient`, mock the actual API calls to avoid external dependencies and ensure test stability. The tests should focus on validating the structure and logic of the `LLMClient` and its provider classes.
- When testing the text generation functionality, assert that the result contains the provider name, model name, and a portion of the prompt.
- When tests involve checking for exceptions related to missing environment variables, use `pytest.raises` to assert that the appropriate `ValueError` is raised and that the exception message is correct.
- When testing the text generation functionality, ensure that the tests account for the stub (placeholder) implementation of the LLMClient. The tests should validate that the expected string (e.g., "OpenAI", model name, and a portion of the prompt) is present in the result, but should not rely on the actual API being called.
- When using `pytest.raises` to test exceptions, if `ty` (or another type checker) reports unresolved attribute errors on `exc_info.value.*`, it may be necessary to narrow the type of `exc_info.value` using one of the following methods:
    - **`isinstance` Check:** Add an `isinstance` assertion after the `with` block to narrow the type of `exc_info.value` before accessing its attributes. For example:
      ```python
      with pytest.raises(LLMError) as exc_info:
          await provider.generate_text(prompt, model)

      err = exc_info.value
      assert isinstance(err, LLMError)
      assert 'OpenAI API呼び出しエラー' in str(err)
      assert err.provider == 'openai'
      assert err.model == model
      assert err.original_error is not None
      ```
    - **`typing.cast`:** Use `typing.cast` to explicitly cast `exc_info.value` to the expected type. For example:
      ```python
      from typing import cast

      with pytest.raises(LLMError) as exc_info:
          await provider.generate_text(prompt, model)

      err = cast(LLMError, exc_info.value)
      assert err.provider == 'openai'
      ```
    - **Type Annotation in Exception Class:** Add type annotations for the attributes in the exception class. For example:
      ```python
      class LLMError(Exception):
          message: str
          provider: str
          model: str
          original_error: Exception | None
          ...
      ```
      This may not fully resolve the type issue but can provide additional clarity.

### Docstring Rules for LLMClient
- Use standard ASCII parentheses in docstrings, not fullwidth parentheses.

### Prompt Management Rules
- The OVERVIEW and REVIEW tools should use prompt management classes.
- Prompt templates should be defined for each tool, and the system should generate prompts dynamically.

### ProjectService Rules
- The `ProjectService` should include AI tool functionality.
- The `_execute_internal_tool` method of the `ProjectService` should use the `PromptManager` and `LLMClient` to generate prompts and execute LLM calls. The implementation should use a synchronous stub of the LLMClient.
- The `ProjectService` should handle exceptions during internal tool execution.
- If the `LLMClient`'s `generate_text` method is asynchronous, the `ProjectService` should be updated to handle asynchronous calls or use a synchronous stub implementation.

### Code Complexity Rules
- When refactoring code, address code complexity issues identified by linters (e.g., `C901` errors for overly complex functions).
- Decompose complex functions into smaller, more manageable sub-functions to improve readability and maintainability.

### Type Annotation Rules
- When defining variables, always provide explicit type annotations to improve code clarity and prevent type-related errors.
- Ensure that type annotations accurately reflect the expected data types, especially when dealing with function arguments and return values.
- When passing keyword arguments (`kwargs`) to functions, perform type checks and casts to ensure that the arguments have the expected types.
- When dealing with `argparse.Namespace` and potential `Any` type inferences, especially when a function requires a specific return type (e.g., `str`), use `typing.cast` to explicitly cast the value to the correct type. This ensures type consistency and satisfies linters. For example:
    ```python
    from typing import cast

    def _parse_app_env(...) -> str:
        ...
        args = parser.parse_args()
        return cast(str, args.app_env)
    ```
- After applying the `cast`, run `just ruff <file_path>` and `just lint` to confirm the code is formatted correctly, and all type checks pass.

### Unused Code Rules
- Regularly check for and remove unused code, including attributes, methods, and variables, to maintain a clean and efficient codebase.
- Use tools like `vulture` to identify potential unused code.

### Test Mocking Rules
- When tests fail with errors like `'Mock' object is not iterable`, review the test's mock setup. Ensure that mocked objects return iterable values when the code under test expects them.

### Integration Testing Rules

- Create integration tests for:
    - **LLMClient and PromptManager:** To ensure they work together correctly.
    - **ProjectService and LLMClient:** To validate the interaction between the service and the LLM client.
- Consider creating end-to-end workflow tests to verify the overall system functionality.

### Phase 5 (Code Cleanup) Rules
- When performing Phase 5 of the project, which involves cleaning up old code:
    - **Remove Old `ait` Command Code:** Remove any code related to the `ait` command, as it's no longer needed.
    - **Remove External AI Tool Code:** Delete any code related to external AI tools, such as `AITool`, `JsonAIToolRepository`, and `AIToolService`.
    - **Verify Data Migration:** Ensure that all data has been successfully migrated to the new internal AI tool system before removing any related code.

### Ruff Specific Rules
- Address code complexity issues identified by linters (e.g., `C901` errors for overly complex functions).
- Fix `RET504 Unnecessary assignment to response before return` errors.
- Fix `UP038 Use X | Y in isinstance call instead of (X, Y)` errors.
- Fix `PLR0911 Too many return statements` errors.
- Fix `ARG005 Unused lambda argument` errors.

### Plan.md Update Rules
- The AI coding assistant should update `plan.md` to reflect the accurate status of the project, including checking completed tasks.

### Task Execution Based on Plan Rules
- When the user instructs the AI to proceed based on `plan.md`, such as "確認し、フェーズ 1 から順に進めてください", the AI coding assistant should:
    - Check `plan.md` to determine the current project status and remaining tasks.
    - Proceed with the tasks outlined in `plan.md` in the order they appear, starting from the specified phase or task.
- When proceeding with tasks from plan.md, if the AI encounters an ambiguous search string due to multiple occurrences of the same string, it should replace the occurrences one by one, confirming each replacement.

### asyncio Replacement Rules
- Replace `asyncio.get_event_loop()` with `asyncio.get_running_loop()`.

### Environment Variable Loading and Configuration Rules

- When the application is launched in test mode, the `.env.test` file should be loaded to configure the environment variables.
- The AI coding assistant should verify that the application correctly loads and utilizes the environment variables defined in `.env.test` when running in test mode.

### Error Handling Rules

- When a `ValueError` occurs due to a missing Python file during the `REVIEW` tool execution, ensure the error message accurately reflects the issue instead of displaying a misleading "Invalid Project ID" message.
- Revise the error handling in `ProjectService._handle_execution_error` to ensure that `ValueError` messages are appropriately conveyed.
- Map `ResourceNotFoundError` to 'プロジェクトが見つかりません: {id}' and ensure `ValueError` returns its string representation.
- Ensure that tests validate that `ResourceNotFoundError` raises the correct message for an invalid project ID, including the ID in the message.
- Ensure that any exception after a failure is saved in `_execute_internal_tool` or adjust `_handle_execution_error` to save even for `ValueError`.
- When the `REVIEW` tool is used with a source directory that does not contain Python files:
    - The AI coding assistant should ensure that a `ValueError` is raised indicating that no Python files were found.
    - The AI coding assistant should suggest that the user either select a directory containing Python files or use the `OVERVIEW` tool for non-code directories.
    - The AI coding assistant should consider updating the UI to prevent the use of the `REVIEW` tool unless the directory contains `.py` files.

### REVIEW Tool File Handling Rules
- The `REVIEW` tool should target files in the specified directory.
- The `REVIEW` tool should prioritize `.md` and `.txt` files. If neither are found, it should fall back to `.py` files.
- If no files of any supported type are found, the `REVIEW` tool should raise a `ValueError` with the message "レビュー対象のファイルが見つかりません".

### .env_sample Rules
- The AI coding assistant should update the comments in the `.env_sample` file to clarify the options for the `LLM_PROVIDER` environment variable. The options are `openai`, `gemini`, and `internal`. If not set, the default is `openai`.
- The AI coding assistant must add the necessary headers for calling the internal LLM to the `.env_sample` file. The specific headers should be based on user input and confirmed with the user.

### LLMClient Provider Name Configuration
- The `provider_name` attribute of the `LLMClient` should be defined as a `StrEnum` to ensure type safety and provide a clear set of valid provider options. The valid options are `openai`, `gemini`, and `internal`.
- The `_initialize_provider` method should use a `match` statement to handle provider-specific initialization based on the `provider_name` enum value.
- For backward compatibility, the externally accessible `provider_name` should remain a string property, while internally the enum should be used.

### Exception Handling Rules

- When raising exceptions, avoid including specific messages directly in the `raise` statement. Instead, pass necessary parameters and store the message within the exception class itself.
- Centralized Logging: Log exception messages at a higher level (e.g., in a central `try...except` block) rather than at the point where the exception is raised. The exception class should provide the message to be logged.
- When raising exceptions within `except` blocks, use `raise ... from err` to preserve the original exception's traceback.

### Internal LLM Header Configuration

- The following fixed headers should be hardcoded into the `InternalLLMProvider` class in `app/utils/llm_client.py`:
    - `accept: application/json`
    - `x-request-type: sync`
    - `x-pool-type: shared`
    - `Content-Type: application/json`
- The `extra_headers` parameter of the `litellm.completion` function should be used to pass these headers.
- When using `litellm` with internal LLMs, ensure that `api_base` and `api_key` are passed to the `litellm.completion` function.
- The `_call_api` method of the `InternalLLMProvider` should add the `custom/` prefix to the model name for internal LLMs, as required by `litellm`.

### Type Definition File Rules
- `ToolType`, `ProjectStatus`, and `LLMProviderName` enums should be defined in `app/types/enums.py`.
- `ProjectID` type should be defined in `app/types/models.py`.
- A type definition package `app/types/` should be created to group all type definitions.
- The `app/types/__init__.py` file should export all StrEnum and custom types.

### Higher-Order Function Docstring Rules
- Higher-order functions (functions that take functions as arguments or return functions) must have Google-style docstrings. See `.cursor/rules/hof_docstrings.mdc` for details.

### Repository Commenting Rules
- The `_normalize_project_data` function in `app/repositories/project_repository.py` must contain comments explaining the intent of data normalization:
    - To exclude unknown keys and calculated properties when reading persistent data back into the model.
    - To adopt only keys existing in `Project.model_fields` to filter out schema-external values.
    - To illustrate that past compatibility fields and calculated properties (e.g., `status`) are filtered out here.

### Public Function Docstring Rules
- Public functions and methods (those not starting with an underscore) must have Google-style docstrings, including summary, Args, Returns, and Raises (if applicable).
- Higher-order functions must also conform to the rules in `.cursor/rules/hof_docstrings.mdc`.
- Private functions and methods (those starting with an underscore) may have docstrings, but they are not required. See `.cursor/rules/public_docstrings.mdc` for details.

### Logging Rules

- In `app/logger.py`, the root logger level should be set according to the `config.LOG_LEVEL`.
- The logging level of `watchdog` should be set to `WARNING` to suppress `DEBUG` logs from `inotify_buffer` and other components.
- In `app.py`, ensure that the logging configuration is initialized by calling `setup_logging()` within the `_initialize_config()` function.
- After initializing the logging configuration, log the initialized log level using `logger.info(f'Logging initialized with level: {config.LOG_LEVEL}')`.

### Internal LLM Configuration Rules
- When using an internal LLM:
    - The `model` parameter should not be passed to `litellm.completion`.
    - The `api_base` and `api_key` parameters must be passed to `litellm.completion`.

### InternalLLMProvider `_call_api` Method Signature Rules

- The `_call_api` method in the `InternalLLMProvider` should accept a `prompt` and a `_model` parameter (unused). The `_model` parameter is required to match the signature of the abstract method in the base class, but it should not be used within the method.

## TECH STACK

- litellm>=1.0.0 # Added based on user decision
## PROJECT DOCUMENTATION & CONTEXT SYSTEM

## CODING STANDARDS

### Exception Handling and Logging

- **Exception Messages:** When raising exceptions, avoid including specific messages directly in the `raise` statement. Instead, pass necessary parameters and store the message within the exception class itself.
- **Centralized Logging:** Log exception messages at a higher level (e.g., in a central `try...except` block) rather than at the point where the exception is raised. The exception class should provide the message to be logged.
- When raising exceptions within `except` blocks, use `raise ... from err` to preserve the original exception's traceback.

### Docstring Formatting
- Avoid using fullwidth parentheses in docstrings. Use standard ASCII parentheses instead.
- Higher-order functions (functions that take functions as arguments or return functions) must have Google-style docstrings. See `.cursor/rules/hof_docstrings.mdc` for details.
- Public functions and methods (those not starting with an underscore) must have Google-style docstrings, including summary, Args, Returns, and Raises (if applicable).
- Higher-order functions must also conform to the rules in `.cursor/rules/hof_docstrings.mdc`.
- Private functions and methods (those starting with an underscore) may have docstrings, but they are not required. See `.cursor/rules/public_docstrings.mdc` for details.

### Type Handling

- When dealing with `argparse.Namespace` and potential `Any` type inferences, especially when a function requires a specific return type (e.g., `str`), use `typing.cast` to explicitly cast the value to the correct type. This ensures type consistency and satisfies linters. For example:
    ```python
    from typing import cast

    def _parse_app_env(...) -> str:
        ...
        args = parser.parse_args()
        return cast(str, args.app_env)
    ```
- After applying the `cast`, run `just ruff <file_path>` and `just lint` to confirm the code is formatted correctly, and all type checks pass.

### Pydantic Configuration

- When using Pydantic, set `model_config = SettingsConfigDict(extra='ignore')` in the `Config` class to allow loading environment variables from `.env` files without causing validation errors due to unknown keys.

## DEBUGGING

- When debugging environment variable issues, the AI coding assistant should:
    - Examine the code in `app/config.py` and `app.py` to ensure the environment is set before importing necessary modules.
    - Use parallel searches for keywords like "DATA_DIR," ".env.test," "APP_ENV," and "dotenv."
    - Search for "load environment variables" to identify where the environment variables are loaded.
    - Identify where "Data directory: ..." is being outputted to determine the source of the incorrect data directory.
    - Read through existing tests, particularly `tests/test_config.py`, to verify the configuration.